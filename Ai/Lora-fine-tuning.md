# 1. âœ… ì˜ˆì‹œ: ê³„ì‚°ê¸°ì™€ ìœ„í‚¤ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ëŠ” Agent
```
from langchain.agents import initialize_agent, Tool
from langchain.tools import DuckDuckGoSearchRun
from langchain.agents.agent_types import AgentType
from langchain.chat_models import ChatOpenAI
from langchain.utilities import SerpAPIWrapper

# íˆ´ ì •ì˜ (DuckDuckGo, ê³„ì‚°ê¸°)
search = DuckDuckGoSearchRun()
tools = [
    Tool(name="Search", func=search.run, description="Useful for web search."),
]

# LLM (OpenAI ì˜ˆì‹œ)
llm = ChatOpenAI(temperature=0)

# Agent ì´ˆê¸°í™”
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# ì‹¤í–‰
agent.run("ì§€ê¸ˆ í•œêµ­ì˜ ëŒ€í†µë ¹ì€ ëˆ„êµ¬ì•¼?")
```

___

# 2. QLoRA vs LoRA
LoRA: ê¸°ì¡´ LLM íŒŒë¼ë¯¸í„°ë¥¼ freezeí•œ ì±„, ì¼ë¶€ weightì—ë§Œ low-rang marixë¥¼ ì¶”ê°€í•˜ëŠ” í•™ìŠµ ë°©ì‹
QLoRA: LoRA ê¸°ë°˜ í•™ìŠµì´ì§€ë§Œ base ëª¨ë¸ì„ 4-bitë¡œ quantizationí•˜ì—¬ í›¨ì”¬ ë” ì‘ì€ vramìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥.


## ğŸ“Š QLoRA vs LoRA ìš”ì•½ ë¹„êµ

|í•­ëª©|LoRA|QLoRA|
|--|--|--|
|Base model precision|FP16, BF16|4-bit quantized (int4)|
|Adapter precision|FP16 / BF16|FP16 / BF16|
|VRAM ì‚¬ìš©ëŸ‰|ì ìŒ|ë§¤ìš° ì ìŒ|
|ì„±ëŠ¥|Full fine-tuningì— ê·¼ì ‘|LoRA ìˆ˜ì¤€ ìœ ì§€|
|êµ¬í˜„ ë³µì¡ë„|ë‚®ìŒ|ì•½ê°„ ë†’ìŒ (bnb, quantization_config í•„ìš”)|
|ëŒ€í‘œ ë¼ì´ë¸ŒëŸ¬ë¦¬|peft|peft + bitsandbytes|


## LoRA config ì„¤ì •
```
# LoRA config 
from peft import LoraConfig

lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
    )
```

## QLoRa config ì„¤ì •
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

# 1. QLoRAìš© ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16  # ë˜ëŠ” float16
)

# 2. ëª¨ë¸ ë¡œë”© (4bit quantized)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",  # ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸
    quantization_config=bnb_config,
    device_map="auto"
)

# 3. LoRA ì„¤ì • ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# 4. LoRA ì ìš©
model = get_peft_model(model, lora_config)
```

___

# 3. QLoRA í™˜ê²½ì—ì„œ RAM ìµœì í™” ì „ëµ

## Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸°
```
from datasets import load_dataset

# 1. streamingìœ¼ë¡œ RAM ì ˆì•½
dataset = load_dataset("your_dataset", split="train", streaming=True)

# 2. map í•  ë•Œ batched=True + remove_columns
dataset = dataset.map(your_preprocess_fn, batched=True, remove_columns=["text", "label"])
```
|ì „ëµ|ì„¤ëª…|
|------|------|
|`streaming=True`|ì „ì²´ ë°ì´í„°ë¥¼ RAMì— ì˜¬ë¦¬ì§€ ì•Šê³ , í•œ ì¤„ì”© ì²˜ë¦¬|
|`remove_columns`|ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì—´ ì œê±°í•˜ì—¬ RAM ì ˆì•½|
|`batched=True`|ì²˜ë¦¬ íšŸìˆ˜ ì¤„ì—¬ ì†ë„ ê°œì„  + ë©”ëª¨ë¦¬ íš¨ìœ¨|


## Tokenizer ì‚¬ìš©ì‹œ ë©”ëª¨ë¦¬ ìµœì í™”
```
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf", use_fast=True)

# 1. padding, truncation ì„¤ì • í•„ìˆ˜
def tokenize_fn(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

```
|ì „ëµ|ì„¤ëª…|
|------|------|
|`trauncation=True`|ê¸´ í…ìŠ¤íŠ¸ë¡œ ì¸í•œ OOM ë°©ì§€ì§€|
|`padding="max_length"`|batching ì •ë ¬ ìµœì í™”í™”|
|`max_length` ì œí•œ|ë©”ëª¨ë¦¬ ê´€ë¦¬ í•µì‹¬!|


## Trainer êµ¬ì„±ì‹œ RAM ê³ ë ¤ ì„¸íŒ…
```
from transformers import TrainingArguments

training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    fp16=True,
    bf16=False,  # bfloat16ì€ GPUì—ì„œë§Œ ì‘ë™, CPU RAMì— ì˜í–¥ ì—†ìŒ
    dataloader_num_workers=2,  # ë„ˆë¬´ ë†’ì´ë©´ RAM ê³¼ë¶€í•˜
    logging_steps=10,
    save_steps=500,
    save_total_limit=2,
    report_to="none",
)
```
|ì „ëµ|ì„¤ëª…|
|------|------|
|`gradient_accumulation`|batch size ëŒ€ì‹  ëˆ„ì  ê³„ì‚°|
|`dataloader_num_workers`|ë§ìœ¼ë©´ ë¹ ë¥´ì§€ë§Œ RAMì„ ë§ì´ ì”€ (2~4 ì¶”ì²œ)|
|`save_total_limit`|ì²´í¬ í¬ì¸íŠ¸ë¡œ RAM/disk ë‚­ë¹„ ë°©ì§€|


## Quantization ì„¤ì • ìµœì í™”
```
BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
```
âœ… RAM ì ˆì•½ íš¨ê³¼
* `double_quant=True`: 2ë‹¨ê³„ ì–‘ìí™”ë¡œ RAM + VRAM ëª¨ë‘ ì ˆì•½  
* `nf4`: ì¼ë°˜ì  NLPì— ì í•©í•œ ë¹„ì†ì‹¤ 4bit quant  
* `compute_dtype=bfloat16`: ì—°ì‚° íš¨ìœ¨ + ë‚®ì€ precision


## í•™ìŠµ ì¤‘ ë©”ëª¨ë¦¬ ìµœì í™”
```
# Gradient Checkpointing í™œì„±í™”
model.gradient_checkpointing_enable()

# Compile ë˜ëŠ” TorchDynamo (ì„ íƒì‚¬í•­)
torch.compile(model)  # PyTorch 2.0 ì´ìƒì—ì„œë§Œ
```
|íš¨ê³¼|ì„¤ëª…|
|--|--|
|Checkpointing|ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì•ˆ í•˜ê³  ë‹¤ì‹œ ê³„ì‚° (RAM ì‚¬ìš©ëŸ‰â†“, ì†ë„â†“)
|torch.compile|ê·¸ë˜í”„ ìµœì í™”ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì¼ ìˆ˜ ìˆìŒ (PyTorch 2.0)


## âœ… ì •ë¦¬ ìš”ì•½

| ì „ëµ | ì„¤ëª… |
|------|------|
| datasets streaming ì‚¬ìš© | ì „ì²´ ë¡œë”© ëŒ€ì‹  í•œ ì¤„ì”© ì²˜ë¦¬ |
| Tokenizer truncation | ë¬´í•œ ë©”ëª¨ë¦¬ ì ìœ  ë°©ì§€ |
| Dataloader num_workers ê°ì†Œ | RAM ì‚¬ìš©ëŸ‰ ê¸‰ê° |
| Gradient checkpointing | RAM/VRAM ë‘˜ ë‹¤ ì¤„ì¼ ìˆ˜ ìˆìŒ |
| BitsAndBytesConfig ì„¸íŒ… ìµœì í™” | ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸ ë¡œë”© ê°€ëŠ¥ |


___

# 4. LoRAì—ì„œ rankëŠ” í•­ìƒ 2ì˜ ì œê³±ìˆ˜ë§Œ ì¨ì•¼í• ê¹Œ?

```
LoraConfig(
    r=64,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
```

ì •ë‹µ: ì•„ë‹ˆë‹¤! í•˜ì§€ë§Œ 2ì˜ ì œê³±ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ”ê²Œ ê°€ì¥ íš¨ê³¼ì ì´ë‹¤.

Loraconfigì˜ `r`íŒŒë¼ë¯¸í„°(rank)ëŠ” ì–´ë–¤ ìˆ˜ ë“  ì…ë ¥í•  ìˆ˜ ìˆë‹¤.  
í•˜ì§€ë§Œ GPU ì—°ì‚° êµ¬ì¡°ìƒ, maxtrix multiply ì—°ì‚°ì—ì„œ 2ì˜ ì œê³±ìˆ˜ í¬ê¸°ê°€ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì´ë‹¤!  
`CUDA`, `cuBLAS`, `TensorCore`ëŠ” 2 4 8 16 32 ë“± **aligned size** ì¼ ë•Œ ì—°ì‚° ìµœì í™”ê°€ ì˜ëœë‹¤!  
ê·¸ë ‡ê¸°ì— í•™ìŠµ ì†ë„ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ ë©´ì—ì„œ 2ì˜ ì œê³± ìˆ˜ê°€ ì„ í˜¸ëœë‹¤.

___

# 5. Aligned sizeê°€ ë­ì•¼?

ë©”ëª¨ë¦¬ë‚˜ ì—°ì‚° ë‹¨ìœ„ê°€ í•˜ë“œì›¨ì–´ì—ì„œ ê°€ì¥ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬ë  ìˆ˜ ìˆë„ë¡ **ì •ë ¬ëœ í¬ê¸°(aligned size)** ë¥¼ ë§í•¨.  

4ë²ˆ ì§ˆë¬¸ì—ì„œ ì—°ê³„ë¡œ ê³„ì† ë‹µë³€í•˜ìë©´
gpuì˜ í…ì„œ ì—°ì‚° ìœ ë‹›ì€ í•œë²ˆì— 16ê°œì˜ floatê°’ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•œë‹¤ê³  ê°€ì •í–ˆì„ ë•Œ  
ì…ë ¥ ë²¡í„°ì˜ ê¸¸ì´ê°€ 17ì´ë¼ë©´??

ì• 16ê°œëŠ” í•œ ë²ˆì— ì²˜ë¦¬ë˜ê³  ë‚˜ë¨¸ì§€ 1ê°œëŠ” ë”°ë¡œ ì²˜ë¦¬ë¨.   
ì´ë•Œ 1ê°œë§Œ ë”°ë¡œ ì²˜ë¦¬ë˜ëŠ” ìœ ë‹›ì—ì„œ 15ê°œì˜ ìŠ¬ë¡¯ì€ ìœ íœ´ ìƒíƒœ(idle cycle)ì„.(ë‚­ë¹„ ë°œìƒ)

ì¦‰, ì´ë¥¼í…Œë©´ input dataë¥¼ 32ê°œì˜ floatë¥¼ ì²˜ë¦¬í•œë‹¤ê³  í•  ë•Œ  
`r`ì„ 17ë¡œ ì„¤ì •í•œë‹¤ë©´  
16 - 1 - 15 ì´ ì„¸ë²ˆì˜ ì—°ì‚°ì´ í•„ìš”í•˜ì§€ë§Œ  

`r`ì„ 16ìœ¼ë¡œ ì„¤ì •í•œë‹¤ë©´  
16 - 16 ë‘ë²ˆì˜ ì—°ì‚°ë§Œ ì§„í–‰ë¨.  


**ë¬¼ë¡  gpu ì•„í‚¤í…ì²˜ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆì–´**   
ì´ë¥¼í…Œë©´ ì‘ì€ ì—°ì‚°ì„ ì—¬ëŸ¬ ì‘ì—…ê³¼ fuse ì‹œí‚¨ë‹¤ë˜ê°€
ë°ì´í„° ë°°ì—´ì„ aligned memory blockìœ¼ë¡œ ì¬ì •ë ¬ í•œë‹¤ë˜ê°€ ë“±





## ğŸ§  ìš”ì•½ í•œ ì¤„
GPU í…ì„œ ìœ ë‹›ì€ ì •í•´ì§„ í¬ê¸° ë‹¨ìœ„ë¡œ ì—°ì‚°í•˜ë©°, ì´ í¬ê¸°ì— ì•ˆ ë§ìœ¼ë©´ ì„±ëŠ¥ ë‚­ë¹„ê°€ ë°œìƒí•œë‹¤.



# 정의
* Vocabulary 내 없는 단어
* UNK 토큰으로 치환되며 의미가 손실되어 정상적인 출력이 어려움.
	* 그렇기에 현대 NLP에서는 `<UNK>` 토큰 사용을 최소화하기 위해 **서브워드 기반**방법을 사용함.

# 현대 NLP의 OOV 해결법 (서브워드 기반)

## BPE (Byte Pair Encoding)
- 원리
	- 희귀 단어를 다루기 위해 단어를 더 작은 단위로 쪼개는 데 사용.
- 동작
	- 가장 자주 함께 등장하는 문자 쌍(pair)을 하나의 새 토큰으로 반복적으로 결합
	- 반복을 거치며 자주 등장하는 단어는 그대로 남고, 희귀 단어는 여러 서브워드로 나뉘어짐.
- 특징
	- 장점: 구현이 쉬우며 희귀 단어를 분해하여 처리하기 때문에 OOV 극복에 용이.
	- 단점: 빈도수 기반으로 단어 쌍을 합치므로 의미 단위와는 다를 수 있음.

## WordPiece
- 원리
	- BERT 등에서 사용하는 방식
	- BPE와 유사하지만, Token 추가 기준이 다름.
	- 전체 말뭉치에서 새로운 조합을 추가하였을 때 Likelihood가 가장 높아지는 조합을 추가함.
- 동작
	- 모든 단어를 문자 단위로 시작
	- 새로운 token 후보를 여러개 만들어봄
	- 후보 중 현재 어휘 집합에 추가했을 때 corpus의 likelihood가 가장 커지는 조합을 어휘에 추가
- 특징
	- BPE보다 **더 의미 있는** 서브워드 생성 가능
	- 어휘 집합이 제한적이어도 새로운 단어를 분해해서 표현 가능


## SentencePiece
- 원리
	- 구글에서 제안한 서브워드 tokenizer
	- BPE, Unigram 등 다양한 분할 알고리즘을 통합적으로 지원
	- 문장 전체를 공백 없이 하나의 문자열로 보고 처리
- 동작
	- 공백(띄어쓰기)도 하나의 문자로 취급
		- 한국어/일본어처럼 띄어쓰기가 불분명한 언어에도 강점
	- BPE 방식 외에도 Unigram Language Model 방식도 지원
		- Unigram 방식은 모든 서브워드 조합 중 가장 높은 확률을 갖는 조합을 찾음
- 특징
	- 띄어쓰기 자체도 서브워드 단위로 처리 
		- 이로 인해 다양한 언어에 대해서 OOV 케이스가 적음.
	- 텍스트를 띄어쓰기 기준으로 split하지 않아도 됨
	- **다양한 분할 알고리즘을 실험적으로 적용**할 수 있음
- 
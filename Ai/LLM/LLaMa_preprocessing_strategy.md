# LLaMa preprocessing 

## 1. Text Cleaning
* ì¤‘ë³µ ì œê±°
    * ê°™ì€ ë‚´ìš©ì´ ì—¬ëŸ¬ë²ˆ í¬í•¨ë  ê²½ìš° overfitting ìœ„í—˜ì´ ìˆìœ¼ë¯€ë¡œ ìœ ì‚¬ ë¬¸ì„œ ì œê±°
    * ì¤‘ë³µ ìœ ì‚¬ë„ ì œê±° ê¸°ì¤€: **MinHash / Locality Senstive Hashing / Cosine Similarity, Jaccard Similarity**

* ê¹¨ì§„ ë¬¸ì / HTML tag / ì»¨íŠ¸ë¡¤ ë¬¸ì ì œê±°
    * `\x00`, `<div>`, `\u200b` ë“±ì˜ ë…¸ì´ì¦ˆ ë¬¸ì ì œê±°

* ê¸´ ê³µë°±, íŠ¹ìˆ˜ ê¸°í˜¸ ë°˜ë³µ ì œê±°
    * `!!!!!!!!`, `------`, `******` ê°™ì€ ë¹„ìì—°ìŠ¤ëŸ¬ìš´ íŒ¨í„´ ì œê±°

* Non-language Text ì œê±°
    * ì½”ë“œ / ë¡œê·¸íŒŒì¼ / ìˆ«ì ë‚˜ì—´ / ë°”ì´ë„ˆë¦¬ ë””ì½”ë”© ë“±ì€ ì œì™¸ë˜ê±°ë‚˜ ë³„ë„ ì²˜ë¦¬


## 2. filtering Bad Samples
* Language Identification
    * ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´ ì œê±° (ex. ì˜ì–´ ëª¨ë¸ì¼ ê²½ìš° í•œêµ­ì–´ ì œê±°)
    * FastText ê¸°ë°˜ ì–¸ì–´ ê°ì§€ ì‚¬ìš©.

* ê¸¸ì´ í•„í„°ë§
    * ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ í•™ìŠµì— ë„ì›€ì´ ì•ˆë˜ë¯€ë¡œ ì‚­ì œ
    * ex. 10 token ì´í•˜ / 2048 token ì´ˆê³¼ ë¬¸ì¥ ì œì™¸

* ë¹„ì†ì–´ / ìœ í•´ ì»¨í…ì¸  í•„í„°ë§
    * Toxic, offensive, harmful ì½˜í…ì¸  ì œê±°
    * ìì²´ ì •ì˜í•œ ê¸ˆì§€ì–´ ë¦¬ìŠ¤íŠ¸
    * HuggingFace Datasets ê¸°ë°˜ í•„í„°ë§ ì‚¬ìš©.

* Low Quality Corpus Filtering
    * í’ˆì§ˆ ë‚®ì€ ë°ì´í„°ì…‹(ex. reddit, forum scraps)ì€ í•„í„°ë§í•˜ê±°ë‚˜ ê°€ì¤‘ì¹˜ë¥¼ ë‚®ê²Œ ë¶€ì—¬


## 3. domain balancing
* í•™ìŠµ ë°ì´í„°ê°€ íŠ¹ì • ë„ë©”ì¸ì— ê³¼í•˜ê²Œ ì¹˜ìš°ì§€ì§€ ì•Šë„ë¡ ë¶„í¬ë¥¼ ì¡°ì •
* ex) Wikipedia 10%, Books 20%, Web Crawl 50%, Academic 20% ë“±ìœ¼ë¡œ êµ¬ì„±.


## Sub-word tokenizer
* BPE(Byte-Pair Encoding) ê¸°ë°˜ SentencePiece tokenizer ì‚¬ìš©
    * LLama 2 > 32k vocabulary BPE ê¸°ë°˜ tokenizer ì‚¬ìš©.
    * Byte-levelìœ¼ë¡œ ë™ì‘í•˜ë¯€ë¡œ í‘œì œì–´/ì–´ê°„ ì¶”ì¶œ ì—†ì´ ë‹¤ì–‘í•œ í˜•íƒœ ì²˜ë¦¬ ê°€ëŠ¥.


___
# 5ï¸âƒ£ ê´€ë ¨ ë…¼ë¬¸ / ê³µì‹ ìë£Œ ë§í¬
## ğŸ“„ ë…¼ë¬¸
LLaMA 1: https://arxiv.org/abs/2302.13971  
â€œLLaMA: Open and Efficient Foundation Language Modelsâ€

LLaMA 2: https://arxiv.org/abs/2307.09288  
â€œLLaMA 2: Open Foundation and Fine-Tuned Chat Modelsâ€

## ğŸ“‚ ëª¨ë¸/í† í¬ë‚˜ì´ì € ì½”ë“œ
HuggingFace Tokenizers: https://github.com/huggingface/tokenizers  
Meta LLaMA GitHub (ê³µì‹ ì½”ë“œ ë¹„ê³µê°œ â†’ ë¦¬íŒ© ë²„ì „):
https://github.com/facebookresearch/llama
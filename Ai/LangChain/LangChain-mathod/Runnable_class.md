# RunnablePassthrough

## ì •ì˜
* no-op(ë¬´ì—°ì‚°) ì²´ì¸
* ì…ë ¥ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥ (ê·¸ ì–´ë–¤ ê°€ê³µX)
* chain ì‚¬ì´ì˜ placeholder ì—­í• 

## ì‚¬ìš© ëª©ì 
* êµ¬í˜„ë˜ì§€ ì•Šì€ ë¶€ë¶„ ì„ì‹œ ì—°ê²°
* íŠ¹ì • Runnable ë™ì‘ì„ ëŠê³  ì‹¶ì„ ë•Œ
* LLM ì• ë’¤ì— ë¶™ì€ chain êµ¬ì„± ìš”ì†Œë¥¼ í…ŒìŠ¤íŠ¸í•  ë•Œ

## ì˜ˆì‹œ ì½”ë“œ
```
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

# 1. Prompt ì •ì˜
prompt = ChatPromptTemplate.from_template("Translate the following English text to French: {text}")

# 2. LangChain LLM
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# 3. RunnablePassthrough ì •ì˜
noop = RunnablePassthrough()

# 4. ì²´ì¸ êµ¬ì„±: Prompt â†’ Passthrough â†’ LLM
chain = prompt | noop | llm

# 5. ì‹¤í–‰
result = chain.invoke({"text": "Hello, how are you?"})
print(result.content)
```
___

# RunnableParallel

## ì •ì˜
* ì…ë ¥ê°’ì„ ë™ì‹œì— ì—¬ëŸ¬ Runnableë¡œ ì „ë‹¬í•˜ì—¬ ê°ê°ì˜ ê²°ê³¼ë¥¼ ë³‘ë ¬ë¡œ ì–»ëŠ” class
* 1ê°œì˜ ì…ë ¥ê°’ -> ë‹¤ìˆ˜ì˜ chain -> ë‹¤ìˆ˜ì˜ output

## ì‚¬ìš© ëª©ì 
* í•˜ë‚˜ì˜ ì…ë ¥ìœ¼ë¡œ ì—¬ëŸ¬ ë°©í–¥ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì—¬ëŸ¬ ì‹œê°ì˜ ê²°ê³¼ë¥¼ ì–»ê³ ìí•  ë•Œ
    * ex) ì…ë ¥ -> ê°ì • ë¶„ì„ + ìš”ì•½ + ë²ˆì—­ 

## ì˜ˆì‹œ ì½”ë“œ
```
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableParallel

# 1. ëª¨ë¸ ì •ì˜
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# 2. í”„ë¡¬í”„íŠ¸ ì •ì˜
summary_prompt = ChatPromptTemplate.from_template("Summarize this text: {text}")
translate_prompt = ChatPromptTemplate.from_template("Translate this to Korean: {text}")

# 3. ê°ê°ì˜ ì²´ì¸ êµ¬ì„±
summary_chain = summary_prompt | llm
translate_chain = translate_prompt | llm

# 4. ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•œ RunnableParallel êµ¬ì„±
parallel_chain = RunnableParallel({
    "summary": summary_chain,
    "translation": translate_chain
})

# 5. ì‹¤í–‰
input_text = {"text": "LangChain is a framework for building applications with LLMs via composable components."}
result = parallel_chain.invoke(input_text)

print("âœ… Summary:", result["summary"].content)
print("âœ… Translation:", result["translation"].content)
```
___

# RunnableMap

## ì •ì˜
* ì…ë ¥ê°’ dictì˜ ê° keyì— ëŒ€í•´ ë‹¤ë¥¸ Runnableì„ mappingí•´ì„œ ì‹¤í–‰í•˜ëŠ” class
* ì…ë ¥ê°’ dictì˜ keyì— ë”°ë¼ ê°ê¸° ë‹¤ë¥¸ chainì´ ì‹¤í–‰ ë¨.

## ì‚¬ìš© ëª©ì 
* input dict keyë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ ë¡œì§(chain)ì„ ì ìš©í•˜ê³  ì‹¶ì„ ë•Œ
* inputì´ ë‹¤ì¤‘ í•„ë“œ(dict)ì¼ ë•Œ

## ì˜ˆì‹œ ì½”ë“œ
```
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableMap

# ëª¨ë¸
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# í”„ë¡¬í”„íŠ¸
summary_prompt = ChatPromptTemplate.from_template("Summarize this: {input}")
translate_prompt = ChatPromptTemplate.from_template("Translate to Korean: {input}")

# ì²´ì¸
summary_chain = summary_prompt | llm
translate_chain = translate_prompt | llm

# RunnableMap êµ¬ì„±: keyë³„ë¡œ inputì´ ë‹¤ë¥´ê²Œ ë§¤í•‘ë¨
map_chain = RunnableMap({
    "summary": summary_chain.with_config(tags=["summary"]),
    "translation": translate_chain.with_config(tags=["translate"]),
})

# ì‹¤í–‰
input_dict = {
    "summary": {"input": "LangChain simplifies working with LLMs by composing chains."},
    "translation": {"input": "LangChain is a useful framework for LLMs."}
}

result = map_chain.invoke(input_dict)

print("ğŸ“ Summary:", result["summary"].content)
print("ğŸ‡°ğŸ‡· Translation:", result["translation"].content)
```

___

# RunnableLambda

## ì •ì˜
* ê°„ë‹¨í•œ Python í•¨ìˆ˜ë¥¼ LangChainì˜ Runnable chainì— í¬í•¨í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” class
* pythonì˜ lambdaì˜ Langchain Runnbale version

## ì‚¬ìš© ëª©ì 
* Runnable ê°ì²´ë¡œ lambdaë¥¼ ì‚¬ìš©í•  ë•Œ

## ì˜ˆì‹œ ì½”ë“œ
```
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# 1. ì‚¬ìš©ì ì •ì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ (ë¬¸ìì—´ì„ ëŒ€ë¬¸ìë¡œ)
uppercase = RunnableLambda(lambda x: {"text": x["text"].upper()})

# 2. í”„ë¡¬í”„íŠ¸ + LLM
prompt = ChatPromptTemplate.from_template("Repeat this: {text}")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# 3. ì²´ì¸ êµ¬ì„±: ëŒ€ë¬¸ì ë³€í™˜ â†’ í”„ë¡¬í”„íŠ¸ â†’ LLM
chain = uppercase | prompt | llm

# 4. ì‹¤í–‰
result = chain.invoke({"text": "hello, langchain lambda"})
print(result.content)
```
___

# RunnableBranch

## ì •ì˜
* ì…ë ¥ê°’ì— ë”°ë¼ ì¡°ê±´ì— ë§ëŠ” runnable chain ì‹¤í–‰
* if/elif/elseì˜ LangChain Runnable version


## ì‚¬ìš© ëª©ì 
* dì…ë ¥ê°’ì— ë”°ë¼ ì„œë¡œ ë‹¤ë¥¸ chainì„ ì‹¤í–‰í•´ì•¼í•  ë•Œ

## ì˜ˆì‹œ ì½”ë“œ
```
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# ê°ê°ì— ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ ì²´ì¸
prompt_greeting = ChatPromptTemplate.from_template("Say hello to the user.")
prompt_qa = ChatPromptTemplate.from_template("Answer this question: {question}")

# ì²´ì¸ ì—°ê²°
greeting_chain = prompt_greeting | llm
question_chain = prompt_qa | llm
default_chain = RunnableLambda(lambda x: "Unknown input")

# ë¶„ê¸° ì²´ì¸ ì •ì˜
branch = RunnableBranch(
    (lambda x: x.get("type") == "greeting", greeting_chain),
    (lambda x: x.get("type") == "question", question_chain),
    default_chain
)

# ì‹¤í–‰
print(branch.invoke({"type": "greeting"}).content)
print(branch.invoke({"type": "question", "question": "What is LangChain?"}).content)
```
___

# RunnableSequence

## ì •ì˜
* ì—¬ëŸ¬ê°œì˜ Runnable ê°ì²´ë¥¼ chainingí•˜ëŠ” class (pipeline êµ¬ì¶•)
* `|` ì—°ì‚°ìì™€ ê°™ì€ ì—­í• . ë‹¨, ì´ëŠ” ëª…ì‹œì ìœ¼ë¡œ ì—¬ëŸ¬ ê°ì²´ë¥¼ chainingí•  ìˆ˜ ìˆìŒ.

## ì‚¬ìš© ëª©ì 
* chainingì„ ì½”ë“œ ìƒì—ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ëª…í™•íˆ êµ¬ì„±í•˜ê³  ì‹¶ì„ ë•Œ
* ë™ì ìœ¼ë¡œ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ì¡°í•©í•˜ê³  ì‹¶ì„ ë•Œ

## ì˜ˆì‹œ ì½”ë“œ
```
from langchain_core.runnables import RunnableSequence

sequence = RunnableSequence([
    chain1,
    chain2,
    chain3
])

result = sequence.invoke(input)
```
___

# RunnableAssign

## ì •ì˜
* dictì„ ì…ë ¥ ë°›ì•„ ì—¬ëŸ¬ Runnableì„ ì‹¤í–‰í•´ì„œ ê·¸ ê²°ê³¼ë¥¼ dict ë‚´ ìƒˆë¡œìš´ key-valueë¡œ ì¶”ê°€í•˜ëŠ” chain
* ì…ë ¥ì„ ê°€ê³µí•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë‘ë˜, ì¶”ê°€ ì •ë³´ë¥¼ ë¶™ì´ëŠ” ìš©ë„

## ì‚¬ìš© ëª©ì 
* ì…ë ¥ ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ, LLM ê²°ê³¼/ë¶„ì„ ë“±ì„ í•„ë“œì— ì¶”ê°€ë¡œ ë„£ê³  ì‹¶ì„ ë•Œ
* ì—¬ëŸ¬ íŒŒìƒ ì •ë³´ë¥¼ ë™ì‹œì— ì¶”ê°€í•˜ê³  ì‹¶ì„ ë•Œ
* ì¤‘ê°„ ì²˜ë¦¬ ë°ì´í„°ë¥¼ ê¸°ë¡í•´ì„œ í›„ì† ì²´ì¸ì—ì„œ ë‹¤ì‹œ ì‚¬ìš©í•˜ê³  ì‹¶ì„ ë•Œ

## ì˜ˆì‹œ ì½”ë“œ
```
from langchain_core.runnables import RunnableLambda, RunnableAssign

# 1. Runnable ì •ì˜
summarizer = RunnableLambda(lambda x: "ìš”ì•½: " + x["text"][:10] + "...")
length_calc = RunnableLambda(lambda x: len(x["text"]))

# 2. Assign ì²´ì¸ êµ¬ì„±
assign = RunnableAssign({
    "summary": summarizer,
    "length": length_calc
})

# 3. ì‹¤í–‰
result = assign.invoke({"text": "LangChainì€ LLMì„ ì—°ê²°í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."})
print(result)
```
```
{
  'text': 'LangChainì€ LLMì„ ì—°ê²°í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.',
  'summary': 'ìš”ì•½: LangChain...',
  'length': 29
}
```
___
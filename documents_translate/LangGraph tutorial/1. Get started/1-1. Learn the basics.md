---
id: -1
subject: 1-1. Learn the basics
last_synced: ""
book_id: 17765
parent_id: 283804
---
src: https://langchain-ai.github.io/langgraph/tutorials/introduction/

# 🚀 LangGraph Quickstart

해당 튜토리얼에서는 LangGraph를 통해 chatbot을 만듭니다.  
LangGraph는 아래와 같은 기능을 가지고 있습니다.

✅ Answer common questions by searching the web  
✅ Maintain conversation state across calls  
✅ Route complex queries to a human for review  
✅ Use custom state to control its behavior  
✅ Rewind and explore alternative conversation paths  


해당 튜토리얼을 통해 기본적인 chatbot을 만들면서
LangGraph의 컨셉을 이해하는 것을 기대할 수 있습니다.

___

## Setup
먼저, 환경설정 및 튜토리얼 진행에 필요한 python package를 설치합니다.
> 해당 튜토리얼은 ANTHROPIC_API가 필요합니다.   
> 단, LLM은 다른 것으로 대체가 가능하니 본인이 가진 api key에 맞게 llm을 선언하여 사용하시길 바랍니다.  

```
%%capture --no-stderr
%pip install -U langgraph langsmith "langchain[anthropic]"
```

```
import getpass
import os


def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


_set_env("ANTHROPIC_API_KEY")
```
>**LangGraph 개발을 위해 [LangSmith](https://smith.langchain.com/)를 설치하세요.**  
LangSmith에 가입하여 LangGraph 프로젝트의 문제를 발견하고 성능을 개선하세요.  
LangSmith는 LangGraph 프로젝트에 사용된 LLM에 대해 debug, test, monitor를 위한 데이터 추적을 지원합니다.  
LangSmith에 대해 더 배우고 싶다면 [여기](https://docs.smith.langchain.com/)를 누르세요.  

<br>

___

## Part 1: 기본적인 Chatbot 구축하기
가장 먼저 LangGraph를 통한 간단한 chatbot을 만들어볼겁니다.  
이 chatbot은 사용자의 messages에 곧바로 응답합니다.  
정말 간단해보이지만 이는 LangGraph의 중요한 특징을 보여줍니다.  
이 챕터를 끝까지 따라오신다면, 당신은 기본적인 chatbot을 훌륭히 만들어낼 수 있습니다.


LangGraph는 `StateGraph`로 부터 시작합니다.  
`StateGraph` 객체는 chatbot 구조를 "state machine"으로 정의합니다.  
또한 `nodes` 객체를 추가합니다.  
`nodes`객체는 llm과 chatbot을 호출할 수 있는 함수 등을 의미합니다.  
마지막으로 `edges`를 추가합니다.  
`edges`는 chatbot이 `nodes`에서 이후 어디로 이동해야할지 정의하는 객체 입니다.  

`StateGraph`: 시작점
`nodes`: 분기점
`edges`: `nodes`에서 llm&chatbot이 어디로 이동할지 정의하는 point

API Reference: [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)
```
from typing import Annotated

from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages


class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)
```

위 graph에서 2가지 작업을 제어할 수 있습니다.
1. 각 `node`는 `State`를 input으로 받을 수 있으며 `State`를 update하여 output으로 출력할 수 있습니다.  
2. messages는 update되면서 덮어씌워지는게 아니라 list에 추가가 될 것입니다. 이는 `Annotated` 문법에 `add_messages` 메소드가 미리 만들어져있기 때문 입니다.  

<br>

>**Concept**  
graph를 정의할 때 가장 첫번째로 `State`를 정의합니다. `State`는 graph의 도식을 포함하고 있으며 `State`를 update할 수 있는 [reducer functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers)도 함게 포함되어 있습니다.  
위 예시를 통해 설명하자면, `State`는 `TypeDict` class를 상속 받았으며 `messages` instance를 가지고 있습니다.  
`add_messages` reducer 함수는 new message를 덮어 싀우는게 아닌 list에 추가하기 위해 사용됩니다.  
핵심은 annotation 없는 recuder는 기존 value를 덮어 씌게 된다는 것입니다.  
state, reducer 그리고 연관된 컨셉을 더 배우고 싶다면 [thist guide](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)를 참고해주세요.  


다음으로는 `chatbot` node를 추가해보도록 하겠습니다.
node들은 각 작업 단위를 의미하며 평범한 python function으로 만들어집니다. 

API Reference: [init_chat_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)

```
from langchain.chat_models import init_chat_model

llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")


def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}


# The first argument is the unique node name
# The second argument is the function or object that will be called whenever
# the node is used.
graph_builder.add_node("chatbot", chatbot)
```

**Notice** 해당 섹션에서는 `chatbot` node 함수가 어떻게 `State`를 입력으로 받고 다시 dictionary type으로 return을 하며, `messages`를 update하는지에 대해 설명합니다. 이러한 패턴은 LangGraph node 함수에서 일반적인 패턴 입니다.

`State`객체에서 사용되는 `add_messages` 메서드는 기존 state에 message에 (어떤 message가 있든 간에) llm의 응답 messages에 추가 합니다.

다음으로는 `entry`point를 추가합니다.  
`entry` point는 우리가 만들고 있는 graph가 어디서 시작하는지 알려줍니다.
```
graph_builder.add_edge(START, "chatbot")
```

위와 비슷하게 `finish`point는 graph가 어디서 종료되는지 알려줍니다.
```
graph_builder.add_edge("chatbot", END)
```

마지막으로, `compile()`함수는 지금까지 만든 우리의 graph가 작동 가능하도록 만듭니다.  
해당 함수는 `CompiledGraph`를 반환하며 우리는 이제부터 이 `CompiledGraph`에 invoke할 수 있습니다.
```
graph = graph_builder.compile()
```

`get_graph`메서드를 사용하면 여러분이 만든 graph를 시각화할 수 있습니다.  
* `get_graph`: `draw_ascii`, `draw_png`와 같은 시각화 메소드 중 하나
* 시각화 메서드는 각각의 추가적인 의존성 패키지 설치를 필요로 한다.
```
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass
```

![[figure1.png]]

이제 chatbot을 실행해보실 수 있습니다!

**Tip**: "quit", "exit", "q"와 같은 입력으로 chat bot 대화를 종료할 수 있습니다. (아래 예시 코드 참고)
```
def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)


while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break
        stream_graph_updates(user_input)
    except:
        # fallback if input() is not available
        user_input = "What do you know about LangGraph?"
        print("User: " + user_input)
        stream_graph_updates(user_input)
        break
```
```
Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It's particularly useful for developing more complex, stateful AI applications that go beyond simple query-response interactions.
Goodbye!
```

**축하합니다.**  
당신은 LangGraph를 통한 첫 chatbot을 만들었습니다.  
해당 chatbot은 사용자의 input을 입력 받아서 LLM으로부터 답변을 반환해주는 기본적인 대화 능력을 가지고 있습니다.  
또한 [LangSmith Trace 링크](https://smith.langchain.com/public/7527e308-9502-4894-b347-f34385740d5a/r)에 접속해보시면 LangSmith로 앞서 호출한 내용에 대한 Trace를 확인해보실 수 있습니다.


### Full Code
API Reference: [init_chat_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)
```
from typing import Annotated

from langchain.chat_models import init_chat_model
from typing_extensions import TypedDict

from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)


llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")


def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}


# The first argument is the unique node name
# The second argument is the function or object that will be called whenever
# the node is used.
graph_builder.add_node("chatbot", chatbot)
graph_builder.set_entry_point("chatbot")
graph_builder.set_finish_point("chatbot")
graph = graph_builder.compile()
```

<br>

___

## Part 2: 🛠️ Tools를 사용하여 Chatbot 성능 향상하기

chatbot이 본인의 기억 만으로 답하지 못하는 질문을 처리하기 위해, 
우리는 웹 검색 tool을 chatbot에 추가할겁니다.  
우리가 만든 chatbot이 해당 tool을 사용한다면, 
웹에서 관련된 정보를 찾고, 더 훌륭한 답변으로 응답할것입니다. 

### 필요한 준비물들

시작하기 이전 웹 검색 tool에 사용될 api key와 추가 package들을 설치해야 합니다.

먼저, [Tavily Search Engine](https://python.langchain.com/docs/integrations/tools/tavily_search/)을 설치한 뒤   [TAVILY_API_KEY](https://tavily.com/)를 설정해줍니다.
```
%%capture --no-stderr
%pip install -U langchain-tavily
```
```
_set_env("TAVILY_API_KEY")
```
```
TAVILY_API_KEY:  YOUR_API_KEY
```

이후 chatbot이 사용할 tool을 정의해줍니다.

API Reference: [TavilySearch](https://python.langchain.com/api_reference/tavily/tavily_search/langchain_tavily.tavily_search.TavilySearch.html)

```
from langchain_tavily import TavilySearch

tool = TavilySearch(max_results=2)
tools = [tool]
tool.invoke("What's a 'node' in LangGraph?")
```
```
{'query': "What's a 'node' in LangGraph?",
 'follow_up_questions': None,
 'answer': None,
 'images': [],
 'results': [{'title': "Introduction to LangGraph: A Beginner's Guide - Medium",
   'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',
   'content': 'Stateful Graph: LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph. We define nodes for classifying the input, handling greetings, and handling search queries. def classify_input_node(state): LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph.',
   'score': 0.7065353,
   'raw_content': None},
  {'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',
   'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',
   'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.',
   'score': 0.5008063,
   'raw_content': None}],
 'response_time': 1.38}
```

위 결과(TavilySearch의 결과)는 chatbot이 질문에 대한 답변을 출력하는데 사용할 수 있습니다.

이후는 LLM(chatbot에 사용하는)에 `bind_tools`을 추가해야합니다. (그 외에는 Part 1에서 했던 것과 동일함.)  
이러한 추가는 LLM이 search engine(TavilSearch)을 사용할 때, LLM이 정확한 JSON 포맷을 사용할 수 있도록 도와줍니다.


API Reference: [init_chat_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)

```
from typing import Annotated

from langchain.chat_models import init_chat_model
from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)


llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")
# Modification: tell the LLM which tools it can call
llm_with_tools = llm.bind_tools(tools)


def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}


graph_builder.add_node("chatbot", chatbot)
```

다음으로, 우리는 함수(LLM이 호출하여 사용할 tool)을 만들어야 합니다.  
만든 함수는 새로운 node에 추가해보도록 하겠습니다.

아래 내용은, 최근 message를 확인하고 message에 tool을 호출하라는 내용이 있다면 해당 tool을 호출하는 `BasicToolNode`를 사용해볼것입니다.
아래 내용은, 최근 message를 확인하고 해당 message에 `tool_calls`가 포함되어 있다면 tool을 호출하는 기능인 `BasicToolNode`에 대해 알아볼 예정입니다.
해당 기능은 사용하는 LLM이 `tool_calling`기능을 지원해야 사용이 가능합니다. (Anthropic, OpenAI, google Gemini를 포함한 기타 LLM) 


이후에는 LangGraph에서 미리 만들어둔 [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode)로 교체하여 실행 속도를 높힐 예정이지만  
지금은 학습을 위해 직접 구현해보도록 하겠습니다.


API Reference: [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html)
```
import json

from langchain_core.messages import ToolMessage


class BasicToolNode:
    """A node that runs the tools requested in the last AIMessage."""

    def __init__(self, tools: list) -> None:
        self.tools_by_name = {tool.name: tool for tool in tools}

    def __call__(self, inputs: dict):
        if messages := inputs.get("messages", []):
            message = messages[-1]
        else:
            raise ValueError("No message found in input")
        outputs = []
        for tool_call in message.tool_calls:
            tool_result = self.tools_by_name[tool_call["name"]].invoke(
                tool_call["args"]
            )
            outputs.append(
                ToolMessage(
                    content=json.dumps(tool_result),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        return {"messages": outputs}


tool_node = BasicToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)
```

tool node가 추가되었으므로, `conditional_edges`를 정의할 수 있습니다.

다시 되짚어보자면, **edges**는 graph의 특정 node에서 다음 node로 이동할 흐름을 제어합니다.  
**Conditional edges**는 입력 받은 graph state를 기준으로 다른 node로 보내기 위해 "if 문"을 포함하며  
다음으로 이동할 node 정보를 나타내는 string 혹은 list를 반환합니다.

아래 내용은, chatbot의 출력에 tool_calls가 포함되어있는지 확인하는 `route_tools`이라 불리는 router 함수를 정의하는 내용입니다.  
이후 `add_conditional_edges`를 호출하여 이 함수(route_tools)를 graph에 등록합니다.
이러한 설정을 통해 chatbot node가 실행을 마칠 때마다, graph가 route_tools 함수를 참고하여 이후 어떤 노드로 이동할지 판단하게 됩니다.  

이 조건은 chatbot 출력에 tool_calls가 있다면 graph state를 `tools`로 보내고 그렇지 않다면 `END`로 보냅니다.

이후에는 이미 만들어져있는 조금 더 간결한 `tools_condition` 함수로 대체할겁니다만,
어떻게 작동하는지 학습하기 위해 직접 만들어보겠습니다.

```
def route_tools(
    state: State,
):
    """
    Use in the conditional_edge to route to the ToolNode if the last message
    has tool calls. Otherwise, route to the end.
    """
    if isinstance(state, list):
        ai_message = state[-1]
    elif messages := state.get("messages", []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "tools"
    return END


# The `tools_condition` function returns "tools" if the chatbot asks to use a tool, and "END" if
# it is fine directly responding. This conditional routing defines the main agent loop.
graph_builder.add_conditional_edges(
    "chatbot",
    route_tools,
    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node
    # It defaults to the identity function, but if you
    # want to use a node named something else apart from "tools",
    # You can update the value of the dictionary to something else
    # e.g., "tools": "my_tools"
    {"tools": "tools", END: END},
)
# Any time a tool is called, we return to the chatbot to decide the next step
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
graph = graph_builder.compile()
```

**Notice** conditional_edge는 1개의 node로부터 시작됩니다.
또한 graph가 상시 chatbot node가 실행될 때마다, 
tool을 호출하면 `tools`로 이동하고, (tool 호출 없이) 직접적으로 응답할 경우 `END`로 이동합니다.

미리 만들어진 `tools_condition`메서드처럼, 우리가 만든 함수도 tool을 호출하지 않는다면 
`END`node로 이동합니다.
`END`node로 이동할 때, 더이상 작업을 수행하지 않으며 실행이 중단됩니다.
router 함수가 `END`를 반환하였고 그에 따라 graph state는 `END`node로 이동하였기 때문에
굳이 우리는 `finish_point`를 명시할 필요 없습니다.
graph는 이미 `finish_point`가 구현되어있습니다.

이제 우리가 만든 graph를 시각화해보도록 하겠습니다.
아래 코드에 대해서는 실행 전에 추가적인 의존성 패키지 설치가 필요합니다.

```
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass
```

![[Pasted image 20250430111717.png]]

이제는 chatbot에게 LLM이 학습한 내용 그 이상의 질문도 할 수 있습니다.

```
while True: [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-2)try: [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-3)user_input = input("User: ") [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-4)if user_input.lower() in ["quit", "exit", "q"]: [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-5)print("Goodbye!") [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-6)break [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-7)[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-8)stream_graph_updates(user_input) [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-9)except: [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-10)# fallback if input() is not available [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-11)user_input = "What do you know about LangGraph?" [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-12)print("User: " + user_input) [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-13)stream_graph_updates(user_input) [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-20-14)break
```

```
Assistant: [{'text': "To provide you with accurate and up-to-date information about LangGraph, I'll need to search for the latest details. Let me do that for you.", 'type': 'text'}, {'id': 'toolu_01Q588CszHaSvvP2MxRq9zRD', 'input': {'query': 'LangGraph AI tool information'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Assistant: [{"url": "https://www.langchain.com/langgraph", "content": "LangGraph sets the foundation for how we can build and scale AI workloads \u2014 from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution ..."}, {"url": "https://github.com/langchain-ai/langgraph", "content": "Overview. LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures ..."}]
Assistant: Based on the search results, I can provide you with information about LangGraph:

1. Purpose:
   LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It's particularly useful for creating agent and multi-agent workflows.

2. Developer:
   LangGraph is developed by LangChain, a company known for its tools and frameworks in the AI and LLM space.

3. Key Features:
   - Cycles: LangGraph allows the definition of flows that involve cycles, which is essential for most agentic architectures.
   - Controllability: It offers enhanced control over the application flow.
   - Persistence: The library provides ways to maintain state and persistence in LLM-based applications.

4. Use Cases:
   LangGraph can be used for various applications, including:
   - Conversational agents
   - Complex task automation
   - Custom LLM-backed experiences

5. Integration:
   LangGraph works in conjunction with LangSmith, another tool by LangChain, to provide an out-of-the-box solution for building complex, production-ready features with LLMs.

6. Significance:
   LangGraph is described as setting the foundation for building and scaling AI workloads. It's positioned as a key tool in the next chapter of LLM-based application development, particularly in the realm of agentic AI.

7. Availability:
   LangGraph is open-source and available on GitHub, which suggests that developers can access and contribute to its codebase.

8. Comparison to Other Frameworks:
   LangGraph is noted to offer unique benefits compared to other LLM frameworks, particularly in its ability to handle cycles, provide controllability, and maintain persistence.

LangGraph appears to be a significant tool in the evolving landscape of LLM-based application development, offering developers new ways to create more complex, stateful, and interactive AI systems.
Goodbye!
```

축하합니다! 
LangGraph를 통해 필요할 때마다 검색 엔진을 통해 필요한 정보를 생성해내는 
대화형 agent 구축에 성공하셨습니다.
이제 조금 더 넓은 범위의 질문을 수행할 수 있게 되었습니다.
또한 앞서 언급했던것처럼 agent의 작업 내용 또한 [LangSmith trace](https://smith.langchain.com/public/4fbd7636-25af-4638-9587-5a02fdbb0172/r)를 통해 추적이 가능합니다.

우리가 만든 chatbot은 여전히 과거 상호작용한 대화에 대해서는 기억하지 못합니다.
또한 일관성 있는 답변과 multi-turn 대화 능력이 부족합니다.
이후 파트에 대해서는 **memory**기능을 추가함으로써 위 기능을 보완해보도록 하겠습니다.

해당 섹션에서 만든 graph의 전체 코드는 아래와 같습니다.
우리가 구현한 `BasicToolNode` 는 [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode)로 대체되었으며
`route_tools`조건 역시 [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#tools_condition)으로 변경되었습니다.

### Full Code
API Reference: [init_chat_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html) | [TavilySearch](https://python.langchain.com/api_reference/tavily/tavily_search/langchain_tavily.tavily_search.TavilySearch.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)
```
from typing import Annotated

from langchain.chat_models import init_chat_model
from langchain_tavily import TavilySearch
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict

from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)


tool = TavilySearch(max_results=2)
tools = [tool]
llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")
llm_with_tools = llm.bind_tools(tools)


def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}


graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
# Any time a tool is called, we return to the chatbot to decide the next step
graph_builder.add_edge("tools", "chatbot")
graph_builder.set_entry_point("chatbot")
graph = graph_builder.compile()
```


___

# Part 3: Chatbot에 Memory 기능 추가하기

현재 우리가 만든 chatbot은 사용자의 질문에 대해 `tools`를 사용하여 답변합니다.
하지만, 과거 대화에 대한 맥락을 기억하진 못합니다.
그렇기에 multi-turn 대화에서 일관적인 답변을 하도록 하는 능력 또한 없습니다.

이러한 문제를 LangGraph에서는 지속적인 `checkpointing`을 통해 해결하였습니다.
graph를 complie할 때 `checkpointer`를 구현하였다면
이후 graph에서 `thread_id`를 통해 이전 대화를 불러올 수 있으며
이로 인해 과거 했던 대화의 중단지점에서부터 다시금 대화를 시작할 수 있습니다.

단순한 chat memory보다 `checkpointing`기능이 훨씬 더 강력하다는 것을 알 수 있을 것이며,
이 기능은 에러 복구, 사용자 개입 기반 workflow, 




`MemorySaver`checkpointer 구현을 시작해보도록 하겠습니다.

API Reference: [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver)
```
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
```
___

# end point